from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ğŸ§  1. Configurar los embeddings para recuperar contexto desde Chroma
embeddings = OllamaEmbeddings(model="mistral")
vectordb = Chroma(persist_directory="embeddings", embedding_function=embeddings)

# ğŸ¤– 2. Cargar el modelo local (Mistral) a travÃ©s de Ollama
llm = Ollama(model="mistral")

# ğŸ§¾ 3. Crear memoria de conversaciÃ³n (guarda preguntas y respuestas previas)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ğŸ” 4. Crear agente RAG con memoria
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectordb.as_retriever(),
    memory=memory,
    verbose=True
)

# ğŸ§ª 5. Bucle interactivo para hacer preguntas al agente con contexto
print("ğŸ’¬ Agente con memoria activado. Escribe 'salir' para terminar.\n")

while True:
    query = input("TÃº: ")
    if query.lower() in ["salir", "exit", "quit"]:
        print("ğŸ‘‹ Hasta luego.")
        break
    query = "Eres un asistente que responde solo en espaÃ±ol, de forma clara y concisa. " + query
    result = qa_chain.run(query)
    print("ğŸ¤–:", result)

