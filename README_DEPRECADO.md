# âš ï¸ Proyecto Finalizado Temporalmente

Este repositorio implementa un agente de IA local utilizando:

- LangChain ğŸ§   
- Ollama (modelo mistral) ğŸ¦™  
- ChromaDB ğŸ—ƒï¸  
- Streamlit (interfaz estilo chat) ğŸ’¬

## âœ… Fases completadas

âœ”ï¸ Fase 1: PreparaciÃ³n del entorno (entorno virtual, dependencias, estructura de carpetas)  
âœ”ï¸ Fase 2: IndexaciÃ³n de conocimiento y embeddings con `mistral` vÃ­a Ollama  
âœ”ï¸ Fase 3: ConstrucciÃ³n del agente conversacional con memoria  
âœ”ï¸ Fase 4: Pruebas funcionales vÃ­a consola  
âœ”ï¸ Fase 5: Interfaz visual tipo WhatsApp con Streamlit

## âŒ Problema detectado

Durante las pruebas finales se descubriÃ³ que **Ollama fue instalado vÃ­a Homebrew**, lo cual:

- Solo instala el cliente CLI (`ollama`)  
- **No incluye ni activa el servicio Ollama** (daemon que escucha en `localhost:11434`)  
- Impide que LangChain o cualquier script pueda consultar modelos como `mistral` localmente

## ğŸ’¡ SoluciÃ³n recomendada

1. Desinstalar Ollama instalado por Homebrew:
   ```bash
   brew uninstall ollama

